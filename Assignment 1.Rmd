---
title: "Using personal activity data to predict successful completion of an exercise"
author: "Christopher Hay"
date: "16 November 2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(cache=TRUE,mesSage=FALSE)
```

##Introduction
The purpose of this document is to list the steps and results of predictive modelling on personal activity data. The aim of the model is to determine whether an activity has been performed succesfully or incorrectly in 5 different ways. The input data was provided from the following website. http://web.archive.org/web/20161224072740/http:/groupware.les.inf.puc-rio.br/har

In this document we perform some initial exploratory analysis and pre-processing, and develop a model via the random forest statistical technique.

##Pre processing 

The data set in question was downloaded from the following location on the 13th of November, 2017.

https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv

20% of the data was split into a validation data test that will be used to determined the out-of-sample error. Of the remaining data, a further 20% was split into a testing data set and the remaining data was used as the training data.

```{r readin}
##download the data and read it into R
##getwd()
setwd("~/Coursera/Course 8 Assignment 1")
##download.file(url="https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv",destfile="training.csv")


pml_data <- read.csv("training.csv")

##observation; pitch is spelled wrong in a few of the variables. We'll leave as is for now but may need to be
##fixed in some of the final model
library(grid)
library(caret)

##split out the data into a training, test and validation data sets
set.seed(1337)
inVal = createDataPartition(pml_data$classe, p = 0.2)[[1]]
pml_data_val <- pml_data[inVal,]
pml_data_build <- pml_data[-inVal,]
set.seed(1338)
inTrain = createDataPartition(pml_data_build$classe,p=0.8)[[1]]
pml_data_train <- pml_data_build[inTrain, ]
pml_data_test <- pml_data_build[-inTrain, ]

dim_pml_data <- dim(pml_data)
dim_pml_data_train <- dim(pml_data_train)
```

The original data set has `r dim_pml_data[1]` rows and  `r dim_pml_data[2]` columns; the on the training data set we have reduced the number of rows to `r dim_pml_data_train[1]` rows.

An initial review of the data column names indicates that a number of the initial variables contain metadata, rather than data that will be useful for the modelling itself. A number of these columns have therefore been removed from the data set, for the below reasons;
raw_timestamp_part_1,raw_timestamp_part_2,cvtd_timestamp - these variables appear to relate to the time the observation was recorded.
user_name; this appears to be the user that entered the data.
new_window,num_window,X; variables that do not appear to have a clear purpose, but do not appear to be related to the question.

```{r subset}
pml_data_train2 <- subset(pml_data_train,select=-c(
                        user_name,raw_timestamp_part_1,raw_timestamp_part_2,cvtd_timestamp
                        ,new_window,num_window,X))
```

A review of the remaining variables reveals some are coded as factors and some as numeric variables. A closer inspection of these factor variables reveals that these are coded thusly due to error values within the data. Some of these values are listed as #div/0! which is an error output from Microsoft Excel. These data items were recoded to null, and then the variables were then recoded to numeric. 

```{r tidy}
varnums <- dim(pml_data_train2)[2]-1


for (i in 1:varnums){
pml_data_train2[,i] <- as.numeric(sub("#DIV/0!","",pml_data_train2[,i]))
}
```
##Exploratory analysis

Before we move to applying any modelling, it is worth getting an understanding of the individual variables within the data. We want to look for how many missing values are in the data by variable, and which variables have the strongest correlation with the output variable.

The below code will, for each of the `r varnums` variables, count the number of missing variables, and fit a multinomial regression model of that variable against the outcome variable. 

```{r explore, echo=FALSE}
library(nnet)
##we cant use standard logistic regression for a variable with multiple classes, but we can use 
##multinomial regression. We'll set up a function to create a one-way model and look at which 
##individual variables are most predictive, based on the AIC.
pml_AICs <- as.data.frame(matrix(data=c(0,0,0,0),1,4))
varnums <- dim(pml_data_train2)[2]-1
for (i in 1:varnums){
    if(sum(is.na(pml_data_train2[,i])) != nrow(pml_data_train2)){

                test_train <- multinom(pml_data_train2$classe~pml_data_train2[,i],trace=FALSE)
        test_train_AIC_i <- c(colnames(pml_data_train2)[i],test_train$AIC,test_train$deviance,sum(is.na(pml_data_train2[,i])))
        pml_AICs[i,] <-test_train_AIC_i
    }
    else
    {test_train_AIC_i <- c(colnames(pml_data_train2)[i],NA,NA,sum(is.na(pml_data_train2[,i])))
    pml_AICs[i,] <-test_train_AIC_i
    }
}
#order the variables
pml_AICs[,2] <- as.numeric(pml_AICs[,2])
pml_AICs[,3] <- as.numeric(pml_AICs[,3])
pml_AICs[,4] <- as.numeric(pml_AICs[,4])
pml_AICs <- pml_AICs[order(pml_AICs[,4],pml_AICs[,2],na.last=TRUE,decreasing=FALSE),]
colnames(pml_AICs) <-c("Var","AIC","Deviance","NAs")

pml_na_table <- table(round(pml_AICs$NAs/dim(pml_data_train2)[1],2))

```

The below graph lists the number of variables by the percentage of missing variables.

```{r plot1}
q1 <- ggplot(data=as.data.frame(pml_na_table),aes(x=Var1,y=Freq))+geom_col()+ggtitle("Variables by percentage of values missing")
q1 <- q1 +ylab("Number of variables")+xlab("Percent of missing values")
q1
```

As we see the variables split into two categories; circa 50 variables where there are no missing data items at all, and then a remainder where a significant percentage of the values are missing. With so many missing values it is unlikely that any kind of data completion strategy will be successful. So, we will proceed with only the variables with no missing values.

The below code strips out the variables with missing values. The same pre processing is then applied to the test and validation sets to allow for model testing later in the process.

```{r noNAs}
pml_NAs <- sapply(pml_data_train2, function(x) sum(is.na(x))==0)
pml_data_train3 <- pml_data_train2[,pml_NAs]


pml_data_test2 <- subset(pml_data_test,select=-c(
    user_name,raw_timestamp_part_1,raw_timestamp_part_2,cvtd_timestamp
    ,new_window,num_window,X))

pml_data_test3 <- pml_data_test2[,pml_NAs]


pml_data_val2 <- subset(pml_data_val,select=-c(
    user_name,raw_timestamp_part_1,raw_timestamp_part_2,cvtd_timestamp
    ,new_window,num_window,X))

pml_data_val3 <- pml_data_val2[,pml_NAs]
```

It is now worth looking at which values were the most predictive based on the single variable multinomial regression applied above. These variables were ranked based on the Akeike Information Coefficient produced from comparing the  
```{r gridExtrax;}
library(gridExtra)
boxplot_train <-list()
for (i in 1:6){
    
    boxplot_train[[i]] <- ggplot(data=pml_data_train2,aes_string(x=c("classe"),y=pml_AICs$Var[i]))+geom_boxplot()+ggtitle(pml_AICs$Var[i])
}

do.call(grid.arrange, c(boxplot_train[1:6], nrow = 3))
```

While it appears there are some relationships between these variables and the outcome variable, it is not immediately clear that any individual gives a good separation of the outcome variable. Therefore, we will need to use more advanced techniques to get a model with good accuracy.

##Modelling. 
We start by fitting a random forest model against the processed training data, using the caret package. We then use the test data set to gain an idea of the accuracy.
```{r rf1}
#enable multi threaded processing
library(parallel)
library(doParallel)
cluster <- makeCluster(detectCores() - 1) # convention to leave 1 core for OS
registerDoParallel(cluster)
set.seed(1001)
pml_rf_cntrl <- trainControl(allowParallel = TRUE)
##fit the random forest
pml_data_rf <- train(data=pml_data_train3,classe~.,method="rf",trControl=pml_rf_cntrl)
stopCluster(cluster)
registerDoSEQ()

##apply the prediction to the test data set and create a confusion matrix.
pml_test_pred_rf <-predict(pml_data_rf,pml_data_test3)
pml_rf_cm <- confusionMatrix(pml_test_pred_rf,pml_data_test3$classe)
```
The random forest method produces a model with accuracy of  `r round(pml_rf_cm$overall[1],3)` with 95% confidence of the accuracy being between `r round(pml_rf_cm$overall[3],3)` and `r round(pml_rf_cm$overall[4],3)`. Below is the actual v predicted results for this model. 

```{r, results='asis'}
knitr::kable(pml_rf_cm$table)
```

Next, we fit a gradient boosting model to the data, and again use the test data set to review the accuracy
of the model.  
```{r gbm }
##enable multi-threaded processing
library(parallel)
library(doParallel)
cluster <- makeCluster(detectCores() - 1) # convention to leave 1 core for OS
registerDoParallel(cluster)
set.seed(1002)
pml_gbm_cntrl <- trainControl(allowParallel = TRUE)
##fit the gradient boosting model 
pml_data_gbm <- train(data=pml_data_train3,classe~.,method="gbm",trControl=pml_gbm_cntrl)

stopCluster(cluster)
registerDoSEQ()

##fit the gradient boosting model against the test data set and create a confusion matrix
pml_test_pred_gbm <-predict(pml_data_gbm,pml_data_test3)
pml_gbm_cm <- confusionMatrix(pml_test_pred_gbm,pml_data_test3$classe)
```
The gradient boosting method produces a model with accuracy of `r round(pml_gbm_cm$overall[1],3)` with 95% confidence of the accuracy being between `r round(pml_gbm_cm$overall[3],3)` and `r round(pml_gbm_cm$overall[4],3)`. Below is the prediction vs actual table for this model.

```{r, results='asis'}
knitr::kable(pml_gbm_cm$table)
```

We can say with at least 95% confidence that the random forest model is more accurate than the gradient boosting model. 

It may be worth testing whether a new random forest model that uses as it's inputs the predictions from the original random forest, and the gradient boosting model. The below code creates this model and then validates it against the test data set.
```{r rf2 }
##fit the new random forest model on the results of the original first 2 models.
pml_train_pred_rf <- predict(pml_data_rf,pml_data_train3)
pml_train_pred_gbm <- predict(pml_data_gbm,pml_data_train3)
pml_data_train_rf2 <- cbind(pml_train_pred_rf,pml_train_pred_gbm,pml_data_train$classe)
colnames(pml_data_train_rf2) <- c("rf","gbm","classe")
cluster <- makeCluster(detectCores() - 1) # convention to leave 1 core for OS
registerDoParallel(cluster)
set.seed(1003)
pml_rf2_cntrl <- trainControl(allowParallel = TRUE)
pml_rf2 <- train(data=pml_data_train_rf2,classe~.,method="rf",trControl=pml_rf2_cntrl)
stopCluster(cluster)
registerDoSEQ()

##apply the prediction to the test data set and create a confusion matrix.
pml_data_test_rf2 <- cbind(pml_test_pred_rf,pml_test_pred_gbm)
colnames(pml_data_test_rf2)[1] <- c("rf")
colnames(pml_data_test_rf2)[2] <- c("gbm")

pml_test_pred_rf <-predict(pml_rf2,pml_data_test_rf2)
pml_test_pred_rf2 <- factor(pml_test_pred_rf,levels = c(1,2,3,4,5),labels=c("A","B","C","D","E"))
pml_rf2_cm <- confusionMatrix(pml_test_pred_rf2,pml_data_test3$classe)
```
This method produces a model with `r round(pml_rf2_cm$overall[1],3)` with 95% confidence of the accuracy being between `r round(pml_rf2_cm$overall[3],3)` and `r round(pml_rf2_cm$overall[4],3)`.

Below is the actual v predicted table for this new random forest model.

```{r, results='asis'}
knitr::kable(pml_rf2_cm$table)
```

While the accuracy is slightly higher the improvement is not statistically significant and so the final recommended model would be the original random forest.

##Out of sample error
Now that we have selected a model we can use the validation data set to get an idea of the out-of-sample error. 

```{r OOS }
pml_val_pred <-predict(pml_data_rf,pml_data_val3)
pml_val_rf_cm <- confusionMatrix(pml_val_pred,pml_data_val3$classe)
```
Based on the validation data set we determine an out-of-sample success rate of `r round(pml_val_rf_cm$overall[1],3)` with 95% confidence of the accuracy being between `r round(pml_val_rf_cm$overall[3],3)` and `r round(pml_val_rf_cm$overall[4],3)`.

##Conclusion
A random forest model against the data appears to produce the best prediction of whether the exercise was performed succesfully or incorrectly in 5 different ways. 